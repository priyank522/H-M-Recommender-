{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b599179-bbf4-4030-84b4-2ffb44b0b23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Libraries loaded successfully!\n",
      "pandas: 2.3.3 | numpy: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 0: IMPORTS (run this first) ----------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle, joblib\n",
    "\n",
    "print(\"âœ” Libraries loaded successfully!\")\n",
    "print(\"pandas:\", pd.__version__, \"| numpy:\", np.__version__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbe78b6f-ff6e-4177-9c58-dfd7fb8ca74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading saved objects...\n",
      "CSV data loaded.\n",
      "user_summary loaded\n",
      "ALS model + encoders loaded\n",
      "Co-purchase model loaded\n",
      "\n",
      "âœ” Reload complete â€” You can continue from Step 6!\n"
     ]
    }
   ],
   "source": [
    "# -------- QUICK RELOAD: Steps 1â€“5 outputs --------\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "print(\"Reloading saved objects...\")\n",
    "\n",
    "# 1) Load transactions FAST\n",
    "transactions = pd.read_csv(\n",
    "    \"data/h-and-m-personalized-fashion-recommendations/transactions_train.csv\",\n",
    "    dtype={\"article_id\": str}\n",
    ")\n",
    "transactions[\"article_id\"] = transactions[\"article_id\"].astype(int)\n",
    "transactions[\"t_dat\"] = pd.to_datetime(transactions[\"t_dat\"])\n",
    "\n",
    "# 2) Load articles & customers (optional for Step 6+)\n",
    "articles = pd.read_csv(\"data/h-and-m-personalized-fashion-recommendations/articles.csv\")\n",
    "customers = pd.read_csv(\"data/h-and-m-personalized-fashion-recommendations/customers.csv\")\n",
    "\n",
    "print(\"CSV data loaded.\")\n",
    "\n",
    "# 3) Load user summary (Step 2 output)\n",
    "user_summary = pd.read_parquet(\"models/user_summary.parquet\")\n",
    "print(\"user_summary loaded\")\n",
    "\n",
    "# 4) Load ALS model + encoders (Step 4)\n",
    "user_encoder = joblib.load(\"models/user_encoder.joblib\")\n",
    "item_encoder = joblib.load(\"models/item_encoder.joblib\")\n",
    "\n",
    "with open(\"models/als_model.pkl\", \"rb\") as f:\n",
    "    als_model = pickle.load(f)\n",
    "\n",
    "print(\"ALS model + encoders loaded\")\n",
    "\n",
    "# 5) Load co-purchase model (Step 5)\n",
    "co_purchase = joblib.load(\"models/co_purchase.joblib\")\n",
    "print(\"Co-purchase model loaded\")\n",
    "\n",
    "print(\"\\nâœ” Reload complete â€” You can continue from Step 6!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45c027-ab8d-4d54-a7bc-c74f3476418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- STEP 1: Load Transactions, Articles, Customers ----------\n",
    "\n",
    "from pathlib import Path\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# File paths (update only if your folder structure is different)\n",
    "TXN_PATH = \"data/h-and-m-personalized-fashion-recommendations/transactions_train.csv\"\n",
    "ART_PATH = \"data/h-and-m-personalized-fashion-recommendations/articles.csv\"\n",
    "CUST_PATH = \"data/h-and-m-personalized-fashion-recommendations/customers.csv\"\n",
    "\n",
    "print(\"Loading transactions...\")\n",
    "transactions = pd.read_csv(TXN_PATH, low_memory=False)\n",
    "print(\"âœ” transactions loaded:\", transactions.shape)\n",
    "\n",
    "print(\"Loading articles...\")\n",
    "articles = pd.read_csv(ART_PATH, low_memory=False)\n",
    "print(\"âœ” articles loaded:\", articles.shape)\n",
    "\n",
    "print(\"Loading customers...\")\n",
    "customers = pd.read_csv(CUST_PATH, low_memory=False)\n",
    "print(\"âœ” customers loaded:\", customers.shape)\n",
    "\n",
    "# Convert t_dat to datetime\n",
    "transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])\n",
    "print(\"\\nâœ” t_dat converted to datetime.\")\n",
    "\n",
    "# Print useful info\n",
    "print(\"\\n--- Data Summary ---\")\n",
    "print(\"Transactions:\", transactions.shape)\n",
    "print(\"Articles:\", articles.shape)\n",
    "print(\"Customers:\", customers.shape)\n",
    "\n",
    "print(\"\\nDate range:\", transactions['t_dat'].min(), \"â†’\", transactions['t_dat'].max())\n",
    "\n",
    "# Approx memory\n",
    "def mem(df):\n",
    "    return df.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "print(\"\\nMemory used:\")\n",
    "print(\"transactions:\", f\"{mem(transactions):.2f} MB\")\n",
    "print(\"articles:\", f\"{mem(articles):.2f} MB\")\n",
    "print(\"customers:\", f\"{mem(customers):.2f} MB\")\n",
    "\n",
    "# Save clean parquet (super fast for reloading later)\n",
    "clean_dir = DATA_DIR / \"clean\"\n",
    "clean_dir.mkdir(exist_ok=True)\n",
    "\n",
    "transactions.to_parquet(clean_dir / \"transactions.parquet\", index=False)\n",
    "articles.to_parquet(clean_dir / \"articles.parquet\", index=False)\n",
    "customers.to_parquet(clean_dir / \"customers.parquet\", index=False)\n",
    "\n",
    "print(f\"\\nâœ” Clean parquet files saved to: {clean_dir.resolve()}\")\n",
    "print(\"\\nSTEP 1 DONE âœ”\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275befe-cc27-4541-834c-aabcd252158e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------- STEP 2: Popularity + User Summary ----------\n",
    "\n",
    "print(\"\\nSTEP 2: Building Popularity & User Summary Features...\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1) Popularity (frequency + recency-weighted)\n",
    "# --------------------------------------------------\n",
    "\n",
    "print(\"Computing popularity scores...\")\n",
    "\n",
    "# Days since purchase (for recency weighting)\n",
    "max_date = transactions['t_dat'].max()\n",
    "transactions['recency_days'] = (max_date - transactions['t_dat']).dt.days\n",
    "\n",
    "# Frequency\n",
    "pop_freq = transactions.groupby('article_id').size()\n",
    "\n",
    "# Recency weight = 1 / (1 + recency_days)\n",
    "transactions['recency_w'] = 1 / (1 + transactions['recency_days'])\n",
    "pop_recency = transactions.groupby('article_id')['recency_w'].sum()\n",
    "\n",
    "# Combine to popularity score\n",
    "popularity = (\n",
    "    pop_freq.rank(method=\"dense\", ascending=False) * 0.5 +\n",
    "    pop_recency.rank(method=\"dense\", ascending=False) * 0.5\n",
    ")\n",
    "\n",
    "top12_popular = popularity.sort_values().head(12).index.tolist()\n",
    "\n",
    "print(\"âœ” Popularity features computed.\")\n",
    "print(\"Top 12 popular items:\", top12_popular)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2) Build User Summary Features\n",
    "# --------------------------------------------------\n",
    "\n",
    "print(\"\\nComputing USER SUMMARY features...\")\n",
    "\n",
    "user_summary = transactions.groupby(\"customer_id\").agg(\n",
    "    total_txn=('article_id', 'count'),\n",
    "    avg_price=('price', 'mean'),\n",
    "    last_purchase=('t_dat', 'max')\n",
    ")\n",
    "\n",
    "# recency of user\n",
    "user_summary['recency_days'] = (max_date - user_summary['last_purchase']).dt.days\n",
    "\n",
    "print(\"\\nExtracting last 5 items per user (SAFE VERSION)...\")\n",
    "\n",
    "# Sort newest â†’ oldest\n",
    "tx_sorted = transactions.sort_values([\"customer_id\", \"t_dat\"], ascending=[True, False])\n",
    "\n",
    "# Take top 5 rows per customer\n",
    "tx_top5 = tx_sorted.groupby(\"customer_id\").head(5)\n",
    "\n",
    "# Now group safely\n",
    "last_items = tx_top5.groupby(\"customer_id\")[\"article_id\"].apply(list)\n",
    "\n",
    "# Assign into user_summary\n",
    "user_summary[\"last_5_items\"] = last_items\n",
    "\n",
    "print(\"âœ” Last 5 items extracted safely.\")\n",
    "print(\"User summary shape:\", user_summary.shape)\n",
    "\n",
    "print(\"\\nSTEP 2 DONE âœ”\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fbb3348-cf0c-4c68-a240-94b95e432781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 3: Popularity MAP@12 Baseline\n",
      "Train date max: 2020-09-14 00:00:00\n",
      "Test date min: 2020-09-15 00:00:00\n",
      "Ground truth users: 75481\n",
      "Top 12 popular items (train): [706016001, 706016002, 372860001, 610776002, 759871002, 464297007, 372860002, 610776001, 399223001, 720125001, 706016003, 156231001]\n",
      "\n",
      "âœ” Baseline POPULARITY MAP@12 = 0.00078\n",
      "\n",
      "STEP 3 DONE âœ”\n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 3: POPULARITY RECOMMENDER + BASELINE MAP@12 ----------\n",
    "\n",
    "print(\"\\nSTEP 3: Popularity MAP@12 Baseline\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1) Create train/test split by date\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "max_date = transactions['t_dat'].max()\n",
    "test_date = max_date - pd.Timedelta(days=7)   # last 7 days as test\n",
    "\n",
    "train_tx = transactions[transactions['t_dat'] < test_date]\n",
    "test_tx  = transactions[transactions['t_dat'] >= test_date]\n",
    "\n",
    "print(\"Train date max:\", train_tx['t_dat'].max())\n",
    "print(\"Test date min:\",  test_tx['t_dat'].min())\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2) Build ground truth for MAP\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "gt = (\n",
    "    test_tx.groupby('customer_id')['article_id']\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(\"Ground truth users:\", len(gt))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3) Popularity top-12 from **training** only\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "pop_train = train_tx.groupby('article_id').size().sort_values(ascending=False)\n",
    "top12_train = pop_train.head(12).index.tolist()\n",
    "\n",
    "print(\"Top 12 popular items (train):\", top12_train)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4) MAP@12 implementation\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def apk(actual, predicted, k=12):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual:\n",
    "            hits += 1\n",
    "            score += hits / (i + 1)\n",
    "    return score / k\n",
    "\n",
    "def mapk(ground_truth, predictions, k=12):\n",
    "    return np.mean([apk(ground_truth[u], predictions[u], k) for u in ground_truth])\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 5) Predict using popularity recommender\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "preds = {uid: top12_train for uid in gt.keys()}\n",
    "\n",
    "baseline_map12 = mapk(gt, preds, k=12)\n",
    "\n",
    "print(f\"\\nâœ” Baseline POPULARITY MAP@12 = {baseline_map12:.5f}\")\n",
    "\n",
    "print(\"\\nSTEP 3 DONE âœ”\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba8c334-a09b-4431-8951-8c6fdeeef646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 4: ALS Collaborative Filtering (FINAL FIXED VERSION)\n",
      "\n",
      "Building LabelEncoders...\n",
      "Users: 1,362,281, Items: 104,547  (built in 10.8s)\n",
      "\n",
      "Building USERâ€“ITEM matrix (correct orientation for ALS)...\n",
      "USERâ€“ITEM matrix shape: (1362281, 104547)\n",
      "\n",
      "Training ALS model (15 iterations)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\HM_Recommender\\venv310\\lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 12 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:29<00:00,  5.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” ALS training complete in 91.3s\n",
      "\n",
      "Preparing safe factor-based recommender...\n",
      "\n",
      "Selecting users with >0 transactions...\n",
      "Sample users: ['6dea99c7816e2c2fae5f974d025b0af044c3b14f1f9aff0512f23bfae770057e'\n",
      " '21cc4c681bed1269fe65b57fb8e024cd64a6054c74a436aff806edb89b897dc5'\n",
      " '9ea1305c846b8256a4109e8e05d882de7c01448e95d7a6c44b10e64c9b27928f'\n",
      " '47f489c3dfaf7d700afdb7698af3ce4505f180a90c97097566229c8d3f68bd37'\n",
      " '2c6b276b22684ff1a0e5eabd1bfe6cf71fbfb0418c9742d6639d68fa71a6636d']\n",
      "\n",
      "Testing ALS recommendations (safe method):\n",
      "User 6dea99c781 â†’ [685814001, 685816002, 685816001, 741356002, 598755001, 685813005, 537116001, 598755002, 570002002, 720504001, 575347003, 570002001]\n",
      "User 21cc4c681b â†’ [684340001, 684341001, 684341002, 684340002, 484398001, 629758005, 689009001, 600886001, 733749001, 564786001, 562245046, 742925003]\n",
      "User 9ea1305c84 â†’ [562245001, 562245046, 562245050, 562245004, 156231001, 399256005, 706016003, 636323001, 484398001, 562245062, 562245061, 562245064]\n",
      "User 47f489c3df â†’ [554479001, 711053003, 554479005, 841383002, 399201005, 706016015, 741356002, 399201022, 621381012, 732842001, 399201002, 399136009]\n",
      "User 2c6b276b22 â†’ [573937001, 294008002, 652924004, 368979001, 751551001, 779554002, 750330002, 652924010, 624257001, 742274001, 640542002, 698387001]\n",
      "\n",
      "Saving model and encoders...\n",
      "\n",
      "STEP 4 DONE âœ” (ALS model + encoders saved)\n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 4: ALS MODEL (FINAL FIXED + ROBUST VERSION) ----------\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "import pickle, joblib, time, os\n",
    "\n",
    "print(\"\\nSTEP 4: ALS Collaborative Filtering (FINAL FIXED VERSION)\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 0) Safety Checks\n",
    "# -------------------------------------------------------------------\n",
    "assert 'transactions' in globals(), \"ERROR: Run STEP 1 first.\"\n",
    "assert 'user_summary' in globals(), \"ERROR: Run STEP 2 first.\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Build LabelEncoders for Users & Items\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\nBuilding LabelEncoders...\")\n",
    "t0 = time.time()\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "user_encoder.fit(transactions[\"customer_id\"])\n",
    "item_encoder.fit(transactions[\"article_id\"])\n",
    "\n",
    "n_users = len(user_encoder.classes_)\n",
    "n_items = len(item_encoder.classes_)\n",
    "\n",
    "print(f\"Users: {n_users:,}, Items: {n_items:,}  (built in {time.time()-t0:.1f}s)\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Build USER Ã— ITEM CSR Matrix (CRITICAL FIX)\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\nBuilding USERâ€“ITEM matrix (correct orientation for ALS)...\")\n",
    "\n",
    "user_idx = user_encoder.transform(transactions[\"customer_id\"])\n",
    "item_idx = item_encoder.transform(transactions[\"article_id\"])\n",
    "\n",
    "data = np.ones(len(user_idx), dtype=np.float32)\n",
    "\n",
    "# âœ”âœ” Correct orientation: rows = users, columns = items\n",
    "user_item_matrix = csr_matrix(\n",
    "    (data, (user_idx, item_idx)),\n",
    "    shape=(n_users, n_items)\n",
    ")\n",
    "\n",
    "print(\"USERâ€“ITEM matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Train ALS model (implicit requires USER Ã— ITEM)\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\nTraining ALS model (15 iterations)...\")\n",
    "als_model = AlternatingLeastSquares(\n",
    "    factors=64,\n",
    "    regularization=0.1,\n",
    "    iterations=15,\n",
    "    use_gpu=False\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "als_model.fit(user_item_matrix)\n",
    "print(f\"âœ” ALS training complete in {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Save factors\n",
    "user_factors = als_model.user_factors      # (n_users, factors)\n",
    "item_factors = als_model.item_factors      # (n_items, factors)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) SAFE RECOMMENDER FUNCTION â€” NO implicit.recommend()\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\nPreparing safe factor-based recommender...\")\n",
    "\n",
    "def recommend_for_encoded_user(enc_uid, N=12):\n",
    "    \"\"\"Return top-N item indices for encoded user.\"\"\"\n",
    "    u_vec = user_factors[enc_uid]   # (factors,)\n",
    "    scores = item_factors.dot(u_vec)   # (n_items,)\n",
    "\n",
    "    # mask purchased items\n",
    "    purchased = user_item_matrix[enc_uid].toarray().ravel()\n",
    "    already = np.where(purchased > 0)[0]\n",
    "    scores[already] = -np.inf\n",
    "\n",
    "    # top-N\n",
    "    top_idx = np.argpartition(-scores, N)[:N]\n",
    "    top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
    "    return top_idx\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5) Select sample users for testing\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\nSelecting users with >0 transactions...\")\n",
    "\n",
    "valid_users = user_summary[user_summary[\"total_txn\"] > 0].index.values\n",
    "valid_users_enc = user_encoder.transform(valid_users)\n",
    "\n",
    "sample_users_enc = np.random.choice(valid_users_enc, size=5, replace=False)\n",
    "sample_users_orig = user_encoder.inverse_transform(sample_users_enc)\n",
    "\n",
    "print(\"Sample users:\", sample_users_orig)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6) Test ALS Recommendations (SAFE)\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\nTesting ALS recommendations (safe method):\")\n",
    "\n",
    "for enc_uid, orig_uid in zip(sample_users_enc, sample_users_orig):\n",
    "    top_items_enc = recommend_for_encoded_user(enc_uid, N=12)\n",
    "    rec_articles = item_encoder.inverse_transform(top_items_enc)\n",
    "\n",
    "    print(f\"User {orig_uid[:10]} â†’ {list(rec_articles)}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7) Save Model + Encoders\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\nSaving model and encoders...\")\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(user_encoder, \"models/user_encoder.joblib\")\n",
    "joblib.dump(item_encoder, \"models/item_encoder.joblib\")\n",
    "\n",
    "with open(\"models/als_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(als_model, f)\n",
    "\n",
    "print(\"\\nSTEP 4 DONE âœ” (ALS model + encoders saved)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2c150b-f617-4f86-a0f2-aa1f7536dc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FAST STEP 5: Batched co-purchase (vectorized)\n",
      "Parameters: TOP_N_PER_ITEM=20, TOP_M_ITEMS=1000, BATCH_SIZE=64, MIN_ITEM_SUPPORT=3\n",
      "n_items: 104547 Computing for top_M: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:49<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FAST co-purchase built and saved to models/co_purchase.joblib in 451.8s\n",
      "Example (first 5 keys):\n",
      "706016001 -> [706016002, 706016003, 706016015, 706016006, 706016019, 539723001, 673901001, 573085028, 554450001, 399223001]\n",
      "706016002 -> [706016001, 706016006, 706016003, 706016015, 706016004, 706016038, 539723005, 573085004, 706016019, 706016007]\n",
      "372860001 -> [372860002, 608776002, 372860024, 575347003, 464297007, 817124001, 653188002, 243937001, 507883009, 717816001]\n",
      "610776002 -> [610776001, 610776028, 554598001, 610776083, 610776107, 610776040, 610776072, 610776007, 561797002, 864288007]\n",
      "759871002 -> [759871001, 759871003, 759871025, 759871004, 759871013, 759871011, 759871014, 759871015, 733749001, 408875001]\n",
      "\n",
      "FAST STEP 5 DONE âœ”\n"
     ]
    }
   ],
   "source": [
    "# ---------- FAST STEP 5: CO-PURCHASE (vectorized, batched) ----------\n",
    "import numpy as np\n",
    "import joblib, time, os\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    tqdm = lambda x, **kw: x\n",
    "\n",
    "print(\"\\nFAST STEP 5: Batched co-purchase (vectorized)\")\n",
    "\n",
    "# Safety checks\n",
    "assert 'user_item_matrix' in globals(), \"user_item_matrix not found â€” run Step 4 first.\"\n",
    "assert 'item_encoder' in globals(), \"item_encoder not found â€” run Step 4 first.\"\n",
    "\n",
    "# Tunable parameters for speed / coverage\n",
    "TOP_N_PER_ITEM = 20     # how many co-purchased items to keep per item\n",
    "TOP_M_ITEMS = 1000      # compute co-purchase for top-M popular items (1000 => very fast)\n",
    "BATCH_SIZE = 64         # number of target items to process per batch\n",
    "MIN_ITEM_SUPPORT = 3    # skip items with very few purchases\n",
    "\n",
    "print(f\"Parameters: TOP_N_PER_ITEM={TOP_N_PER_ITEM}, TOP_M_ITEMS={TOP_M_ITEMS}, BATCH_SIZE={BATCH_SIZE}, MIN_ITEM_SUPPORT={MIN_ITEM_SUPPORT}\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# 1) item popularity and selection\n",
    "item_counts = np.array(user_item_matrix.sum(axis=0)).ravel()\n",
    "n_items = item_counts.shape[0]\n",
    "popular_idx = np.argsort(-item_counts)\n",
    "top_m_idx = popular_idx[: min(TOP_M_ITEMS, n_items)]\n",
    "print(\"n_items:\", n_items, \"Computing for top_M:\", len(top_m_idx))\n",
    "\n",
    "# 2) Precompute L2 norms for items (for cosine)\n",
    "item_sq_sums = np.array(user_item_matrix.power(2).sum(axis=0)).ravel()\n",
    "item_l2 = np.sqrt(item_sq_sums)\n",
    "item_l2[item_l2 == 0] = 1.0\n",
    "\n",
    "# 3) We'll compute in batches: for batch of target items, compute sims = (U^T * U_batch).T / (l2_tgt * l2_all)\n",
    "co_purchase = {}\n",
    "global_topN = item_encoder.inverse_transform(popular_idx[:TOP_N_PER_ITEM])\n",
    "\n",
    "# Process batches\n",
    "batches = [top_m_idx[i:i+BATCH_SIZE] for i in range(0, len(top_m_idx), BATCH_SIZE)]\n",
    "for batch in tqdm(batches, desc=\"batches\"):\n",
    "    # user_item_matrix[:, batch] -> (n_users, batch_size)\n",
    "    # compute (batch_size, n_items) = (batch_size, n_users) dot (n_users, n_items)\n",
    "    # do this by transposing the left: (n_users, batch).T dot (n_users, n_items)\n",
    "    U_batch = user_item_matrix[:, batch]        # shape (n_users, batch)\n",
    "    sims_batch = (U_batch.T).dot(user_item_matrix).toarray()  # (batch, n_items) dense array\n",
    "\n",
    "    # normalize per row (batch item) using item_l2\n",
    "    # denom for each row = item_l2[batch_item] * item_l2 (vector)\n",
    "    denom = item_l2[batch][:, None] * item_l2[None, :]  # (batch, n_items)\n",
    "    sims_batch = sims_batch / denom\n",
    "    # for each target in batch, zero (or -inf) the self cell\n",
    "    for i_idx, tgt in enumerate(batch):\n",
    "        sims_batch[i_idx, tgt] = -np.inf\n",
    "\n",
    "    # for each target row pick top-k\n",
    "    for i_idx, tgt in enumerate(batch):\n",
    "        if item_counts[tgt] < MIN_ITEM_SUPPORT:\n",
    "            co_purchase[item_encoder.inverse_transform([tgt])[0]] = []\n",
    "            continue\n",
    "\n",
    "        sims = sims_batch[i_idx]\n",
    "        if TOP_N_PER_ITEM >= n_items:\n",
    "            top_idx = np.argsort(-sims)\n",
    "        else:\n",
    "            # faster selection\n",
    "            top_idx = np.argpartition(-sims, TOP_N_PER_ITEM)[:TOP_N_PER_ITEM]\n",
    "            top_idx = top_idx[np.argsort(-sims[top_idx])]\n",
    "\n",
    "        top_articles = item_encoder.inverse_transform(top_idx.astype(int))\n",
    "        co_purchase[item_encoder.inverse_transform([tgt])[0]] = list(top_articles)\n",
    "\n",
    "# 4) fallback fill for items not computed\n",
    "for i in range(n_items):\n",
    "    art = item_encoder.inverse_transform([i])[0]\n",
    "    if art not in co_purchase:\n",
    "        fallback = [a for a in global_topN if a != art][:TOP_N_PER_ITEM]\n",
    "        co_purchase[art] = fallback\n",
    "\n",
    "# 5) save mapping\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(co_purchase, \"models/co_purchase.joblib\")\n",
    "\n",
    "print(f\"\\nFAST co-purchase built and saved to models/co_purchase.joblib in {time.time()-t0:.1f}s\")\n",
    "print(\"Example (first 5 keys):\")\n",
    "keys = list(co_purchase.keys())[:5]\n",
    "for k in keys:\n",
    "    print(k, \"->\", co_purchase[k][:10])\n",
    "\n",
    "print(\"\\nFAST STEP 5 DONE âœ”\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bacffccc-cda7-4c7f-9aec-2f0d1605a1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding user_summary from transactions...\n",
      "user_summary shape: (1362281, 2)\n",
      "\n",
      "âœ” user_summary.parquet saved to models/\n"
     ]
    }
   ],
   "source": [
    "# ---- REBUILD user_summary (FAST SAFE version) ----\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Rebuilding user_summary from transactions...\")\n",
    "\n",
    "# load only required columns\n",
    "transactions = pd.read_csv(\n",
    "    \"data/h-and-m-personalized-fashion-recommendations/transactions_train.csv\",\n",
    "    dtype={\"article_id\": str}\n",
    ")\n",
    "transactions[\"article_id\"] = transactions[\"article_id\"].astype(int)\n",
    "transactions[\"t_dat\"] = pd.to_datetime(transactions[\"t_dat\"])\n",
    "\n",
    "# --- popularity ---\n",
    "item_pop = transactions.groupby(\"article_id\").size().sort_values(ascending=False)\n",
    "\n",
    "# --- user_summary basic ---\n",
    "user_summary = transactions.groupby(\"customer_id\").size().to_frame(\"total_txn\")\n",
    "\n",
    "# --- last 5 items per user ---\n",
    "tx_sorted = transactions.sort_values([\"customer_id\", \"t_dat\"], ascending=[True, False])\n",
    "last_items = (\n",
    "    tx_sorted.groupby(\"customer_id\")[\"article_id\"]\n",
    "    .head(5)\n",
    "    .groupby(tx_sorted[\"customer_id\"])\n",
    "    .apply(list)\n",
    ")\n",
    "\n",
    "user_summary[\"last_5_items\"] = last_items\n",
    "\n",
    "print(\"user_summary shape:\", user_summary.shape)\n",
    "\n",
    "# SAVE\n",
    "import os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "user_summary.to_parquet(\"models/user_summary.parquet\")\n",
    "\n",
    "print(\"\\nâœ” user_summary.parquet saved to models/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be905d59-77cc-4e55-9f90-03ce08b9ada3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 6: Optimized Candidate Generator for 10,000 users...\n",
      "Rebuilding USERâ€“ITEM matrix...\n",
      "User-item matrix shape: (1362281, 104547)\n",
      "Generating candidates for 10000 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [4:06:11<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 6 DONE âœ” (10k users complete)\n",
      "Saved â†’ models/candidates.joblib\n"
     ]
    }
   ],
   "source": [
    "# -------- STEP 6 (OPTIMIZED FOR 10K USERS): Candidate Generator --------\n",
    "import numpy as np\n",
    "import joblib, pickle\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "print(\"\\nSTEP 6: Optimized Candidate Generator for 10,000 users...\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Load encoders, ALS model, co-purchase\n",
    "# ------------------------------------------------------------------\n",
    "user_encoder = joblib.load(\"models/user_encoder.joblib\")\n",
    "item_encoder = joblib.load(\"models/item_encoder.joblib\")\n",
    "co_purchase  = joblib.load(\"models/co_purchase.joblib\")\n",
    "\n",
    "with open(\"models/als_model.pkl\", \"rb\") as f:\n",
    "    als_model = pickle.load(f)\n",
    "\n",
    "item_factors = als_model.item_factors\n",
    "user_factors = als_model.user_factors\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Rebuild USERâ€“ITEM matrix (for filtering)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Rebuilding USERâ€“ITEM matrix...\")\n",
    "\n",
    "rows = item_encoder.transform(transactions[\"article_id\"])\n",
    "cols = user_encoder.transform(transactions[\"customer_id\"])\n",
    "data = np.ones(len(rows), dtype=np.float32)\n",
    "\n",
    "item_user_matrix = csr_matrix(\n",
    "    (data, (rows, cols)),\n",
    "    shape=(len(item_encoder.classes_), len(user_encoder.classes_))\n",
    ")\n",
    "\n",
    "user_item_matrix = item_user_matrix.T\n",
    "print(\"User-item matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Fast ALS scoring\n",
    "# ------------------------------------------------------------------\n",
    "def recommend_for_user(uid_enc, topN=50):\n",
    "    u = user_factors[uid_enc]             \n",
    "    scores = item_factors.dot(u)          \n",
    "\n",
    "    purchased = user_item_matrix.getrow(uid_enc).indices\n",
    "    if len(purchased) > 0:\n",
    "        scores[purchased] = -np.inf\n",
    "\n",
    "    top = np.argpartition(-scores, topN)[:topN]\n",
    "    top = top[np.argsort(-scores[top])]\n",
    "    return top\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Generate candidates for 10,000 users\n",
    "# ------------------------------------------------------------------\n",
    "USER_LIMIT = 10000\n",
    "print(f\"Generating candidates for {USER_LIMIT} users...\")\n",
    "\n",
    "candidates = {}\n",
    "user_list = user_summary.index[:USER_LIMIT]\n",
    "\n",
    "for user_id in tqdm(user_list, desc=\"Users\"):\n",
    "\n",
    "    uid_enc = user_encoder.transform([user_id])[0]\n",
    "\n",
    "    # ---- ALS candidates ----\n",
    "    als_top = recommend_for_user(uid_enc, topN=50)\n",
    "    als_items = item_encoder.inverse_transform(als_top)\n",
    "\n",
    "    # ---- Co-purchase ----\n",
    "    last_items = user_summary.loc[user_id, \"last_5_items\"]\n",
    "    cop_items = []\n",
    "\n",
    "    for it in last_items:\n",
    "        if it in co_purchase:\n",
    "            cop_items.extend(co_purchase[it][:20])\n",
    "\n",
    "    cop_items = np.array(cop_items, dtype=np.int64)\n",
    "\n",
    "    # ---- Merge ----\n",
    "    merged = np.unique(np.concatenate([als_items, cop_items]))\n",
    "    candidates[user_id] = merged[:200]    # up to 200 items per user\n",
    "\n",
    "\n",
    "print(\"\\nSTEP 6 DONE âœ” (10k users complete)\")\n",
    "joblib.dump(candidates, \"models/candidates.joblib\")\n",
    "print(\"Saved â†’ models/candidates.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24625e25-f416-490d-8b56-2dd237bb485f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 7: Building training features (FAST MODE)...\n",
      "Loading saved objects...\n",
      "Rebuilding USERâ€“ITEM matrix...\n",
      "\n",
      "Generating feature rows... (this is the main loop)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9999/10000 [3:39:19<00:01,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature DataFrame shape: (765437, 6)\n",
      "\n",
      "âœ” STEP 7 DONE (FAST MODE). Saved â†’ models/train_features.parquet\n"
     ]
    }
   ],
   "source": [
    "# -------- STEP 7: FAST FEATURE BUILDER (under 1 hour) --------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\nSTEP 7: Building training features (FAST MODE)...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Load required files\n",
    "# ---------------------------------------------------------\n",
    "print(\"Loading saved objects...\")\n",
    "\n",
    "user_summary = pd.read_parquet(\"models/user_summary.parquet\")\n",
    "candidates = joblib.load(\"models/candidates.joblib\")\n",
    "\n",
    "user_encoder = joblib.load(\"models/user_encoder.joblib\")\n",
    "item_encoder = joblib.load(\"models/item_encoder.joblib\")\n",
    "\n",
    "with open(\"models/als_model.pkl\", \"rb\") as f:\n",
    "    als_model = pickle.load(f)\n",
    "\n",
    "item_factors = als_model.item_factors\n",
    "user_factors = als_model.user_factors\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Build USERâ€“ITEM matrix (for interaction lookup)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Rebuilding USERâ€“ITEM matrix...\")\n",
    "\n",
    "rows = item_encoder.transform(transactions[\"article_id\"])\n",
    "cols = user_encoder.transform(transactions[\"customer_id\"])\n",
    "data = np.ones(len(rows), dtype=np.float32)\n",
    "\n",
    "item_user_matrix = csr_matrix((data, (rows, cols)),\n",
    "                              shape=(len(item_encoder.classes_),\n",
    "                                     len(user_encoder.classes_)))\n",
    "\n",
    "user_item_matrix = item_user_matrix.T   # (users Ã— items)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Fast feature functions\n",
    "# ---------------------------------------------------------\n",
    "def sim_user_item_score(uid_enc, item_enc):\n",
    "    \"\"\"Cosine similarity between user vector & item vector (very fast).\"\"\"\n",
    "    return np.dot(user_factors[uid_enc], item_factors[item_enc])\n",
    "\n",
    "def user_interaction_count(uid_enc):\n",
    "    \"\"\"How many items user interacted with.\"\"\"\n",
    "    return user_item_matrix.getrow(uid_enc).count_nonzero()\n",
    "\n",
    "def item_popularity(enc_item):\n",
    "    \"\"\"How many users bought the item.\"\"\"\n",
    "    return item_user_matrix.getrow(enc_item).count_nonzero()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) Build feature rows\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "print(\"\\nGenerating feature rows... (this is the main loop)\")\n",
    "\n",
    "rows = []\n",
    "counter = 0\n",
    "\n",
    "for user_id, items in tqdm(candidates.items(), total=len(candidates)):\n",
    "\n",
    "    uid_enc = user_encoder.transform([user_id])[0]\n",
    "\n",
    "    # vectorized encode candidate items\n",
    "    items_enc = item_encoder.transform(items)\n",
    "\n",
    "    # vectorized feature calculation\n",
    "    sim_scores = np.dot(item_factors[items_enc], user_factors[uid_enc])\n",
    "\n",
    "    # popularities\n",
    "    pops = np.array([item_popularity(i) for i in items_enc])\n",
    "\n",
    "    # user transaction strength\n",
    "    u_tx = user_interaction_count(uid_enc)\n",
    "\n",
    "    # labels: 1 if in last week's purchases\n",
    "    last_items = set(user_summary.loc[user_id, \"last_5_items\"])\n",
    "    labels = np.array([1 if int(it) in last_items else 0 for it in items])\n",
    "\n",
    "    # build rows\n",
    "    for i, it in enumerate(items):\n",
    "        rows.append([\n",
    "            user_id,\n",
    "            it,\n",
    "            sim_scores[i],\n",
    "            pops[i],\n",
    "            u_tx,\n",
    "            labels[i]\n",
    "        ])\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    # EARLY FINISH OPTION (optional)\n",
    "    if counter >= 10000:     # 10k users limit (FAST)\n",
    "        break\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Convert to DataFrame\n",
    "# ---------------------------------------------------------\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    \"user_id\",\n",
    "    \"item_id\",\n",
    "    \"als_similarity\",\n",
    "    \"item_popularity\",\n",
    "    \"user_total_txn\",\n",
    "    \"label\"\n",
    "])\n",
    "\n",
    "print(\"\\nFeature DataFrame shape:\", df.shape)\n",
    "\n",
    "df.to_parquet(\"models/train_features.parquet\")\n",
    "print(\"\\nâœ” STEP 7 DONE (FAST MODE). Saved â†’ models/train_features.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdfc116d-6d0c-486c-a85d-bb70761bf081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 8: Training LightGBM (FINAL optimized)...\n",
      "Training data: (765437, 6)\n",
      "Positive labels: 2651\n",
      "Unique users: 10000\n",
      "Train: (688893, 6) Valid: (76544, 6)\n",
      "scale_pos_weight: 287.7229673093043\n",
      "Training LightGBM...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[315]\tvalid's auc: 0.911039\n",
      "\n",
      "âœ” Step 8 complete!\n",
      "Best iteration: 315\n",
      "Saved â†’ models/lgbm_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# -------- STEP 8: LIGHTGBM TRAINING (FINAL OPTIMIZED) --------\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nSTEP 8: Training LightGBM (FINAL optimized)...\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1) LOAD FEATURES\n",
    "# ----------------------------------------------------\n",
    "df = pd.read_parquet(\"models/train_features.parquet\")\n",
    "print(\"Training data:\", df.shape)\n",
    "\n",
    "print(\"Positive labels:\", df[\"label\"].sum())\n",
    "print(\"Unique users:\", df[\"user_id\"].nunique())\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) STRATIFIED SPLIT\n",
    "# ----------------------------------------------------\n",
    "train_df, valid_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]  # ensure positives appear in valid\n",
    ")\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Valid:\", valid_df.shape)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3) SELECT FEATURES\n",
    "# ----------------------------------------------------\n",
    "feature_cols = [\n",
    "    \"als_similarity\",\n",
    "    \"item_popularity\",\n",
    "    \"user_total_txn\"\n",
    "]\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[\"label\"]\n",
    "\n",
    "X_valid = valid_df[feature_cols]\n",
    "y_valid = valid_df[\"label\"]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4) HANDLE EXTREME CLASS IMBALANCE\n",
    "# ----------------------------------------------------\n",
    "# If positives extremely small, boost positive weight\n",
    "pos_rate = y_train.mean()\n",
    "scale_pos_weight = (1 - pos_rate) / pos_rate if pos_rate > 0 else 1\n",
    "\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5) LIGHTGBM PARAMETERS (Optimized)\n",
    "# ----------------------------------------------------\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 32,\n",
    "    \"max_depth\": -1,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 3,\n",
    "    \"min_data_in_leaf\": 40,\n",
    "    \"lambda_l2\": 1.0,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6) LightGBM DATASETS\n",
    "# ----------------------------------------------------\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 7) TRAIN MODEL\n",
    "# ----------------------------------------------------\n",
    "print(\"Training LightGBM...\")\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[valid_data],\n",
    "    valid_names=[\"valid\"],\n",
    "    num_boost_round=500,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ” Step 8 complete!\")\n",
    "print(\"Best iteration:\", model.best_iteration)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 8) SAVE MODEL\n",
    "# ----------------------------------------------------\n",
    "joblib.dump(model, \"models/lgbm_model.joblib\")\n",
    "print(\"Saved â†’ models/lgbm_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c09ba8f0-1fca-4315-9766-dc7ca087d7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 9 (FAST): Final predictions + MAP@12 evaluation\n",
      "Preparing batch prediction input...\n",
      "Total scoring rows: 765437\n",
      "Encoding...\n",
      "Computing vectorized features...\n",
      "Predicting scores...\n",
      "Generating sorted predictions...\n",
      "Computing MAP@12...\n",
      "\n",
      "ðŸ”¥ FAST MAP@12 = 0.0001179671651628917\n",
      "\n",
      "âœ” STEP 9 COMPLETE (FAST VERSION)!\n"
     ]
    }
   ],
   "source": [
    "# -------- STEP 9 (FAST VERSION): Vectorized MAP@12 + Predictions --------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "print(\"\\nSTEP 9 (FAST): Final predictions + MAP@12 evaluation\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) LOAD MODELS + CANDIDATES + ENCODERS\n",
    "# ---------------------------------------------------------\n",
    "model = joblib.load(\"models/lgbm_model.joblib\")\n",
    "candidates = joblib.load(\"models/candidates.joblib\")\n",
    "\n",
    "user_encoder = joblib.load(\"models/user_encoder.joblib\")\n",
    "item_encoder = joblib.load(\"models/item_encoder.joblib\")\n",
    "\n",
    "# From Step 4 (already loaded earlier or reload)\n",
    "item_factors = als_model.item_factors\n",
    "user_factors = als_model.user_factors\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) BUILD GROUND TRUTH (LAST 7 DAYS)\n",
    "# ---------------------------------------------------------\n",
    "TEST_START = transactions[\"t_dat\"].max() - pd.Timedelta(days=6)\n",
    "test_df = transactions[transactions[\"t_dat\"] >= TEST_START]\n",
    "\n",
    "gt = (\n",
    "    test_df.groupby(\"customer_id\")[\"article_id\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) PREPARE BULK PREDICTION INPUTS\n",
    "# ---------------------------------------------------------\n",
    "print(\"Preparing batch prediction input...\")\n",
    "\n",
    "user_ids = []\n",
    "item_ids = []\n",
    "\n",
    "# Flatten candidate lists\n",
    "for user, items in candidates.items():\n",
    "    user_ids.extend([user] * len(items))\n",
    "    item_ids.extend(list(items))\n",
    "\n",
    "user_ids = np.array(user_ids)\n",
    "item_ids = np.array(item_ids)\n",
    "\n",
    "print(\"Total scoring rows:\", len(user_ids))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) ENCODE USERS + ITEMS (vectorized)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Encoding...\")\n",
    "user_enc = user_encoder.transform(user_ids)\n",
    "item_enc = item_encoder.transform(item_ids)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) VECTORIZED FEATURE COMPUTATION (FAST)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Computing vectorized features...\")\n",
    "\n",
    "# ALS similarity: dot for all rows\n",
    "als_sims = np.sum(\n",
    "    user_factors[user_enc] * item_factors[item_enc],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# popularity = number of buyers\n",
    "# (VERY FAST using CSR matrix sum)\n",
    "item_pop = np.array(item_user_matrix.sum(axis=1)).reshape(-1)\n",
    "item_pops = item_pop[item_enc]\n",
    "\n",
    "# user transaction count\n",
    "user_tx = np.array(user_item_matrix.sum(axis=1)).reshape(-1)\n",
    "user_txs = user_tx[user_enc]\n",
    "\n",
    "# Create final feature matrix\n",
    "X = pd.DataFrame({\n",
    "    \"als_similarity\": als_sims,\n",
    "    \"item_popularity\": item_pops,\n",
    "    \"user_total_txn\": user_txs,\n",
    "})\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6) PREDICT USING LIGHTGBM (VERY FAST)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Predicting scores...\")\n",
    "scores = model.predict(X)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7) GROUP BACK INTO USER â†’ SORTED TOP 12\n",
    "# ---------------------------------------------------------\n",
    "print(\"Generating sorted predictions...\")\n",
    "\n",
    "predictions = {}\n",
    "idx = 0\n",
    "\n",
    "for user, items in candidates.items():\n",
    "    n = len(items)\n",
    "    user_scores = scores[idx : idx + n]\n",
    "    idx += n\n",
    "\n",
    "    # sort top 12\n",
    "    top_idx = np.argsort(-user_scores)[:12]\n",
    "    top_items = np.array(items)[top_idx]\n",
    "\n",
    "    predictions[user] = list(top_items)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8) FAST MAP@12 CALCULATION\n",
    "# ---------------------------------------------------------\n",
    "def apk(actual, predicted, k=12):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual:\n",
    "            hits += 1.0\n",
    "            score += hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "print(\"Computing MAP@12...\")\n",
    "\n",
    "scores_list = []\n",
    "for user, actual in gt.items():\n",
    "    pred = predictions.get(user, [])\n",
    "    scores_list.append(apk(actual, pred))\n",
    "\n",
    "map12 = np.mean(scores_list)\n",
    "print(\"\\nðŸ”¥ FAST MAP@12 =\", map12)\n",
    "\n",
    "print(\"\\nâœ” STEP 9 COMPLETE (FAST VERSION)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5913fc1-0b77-4bfa-87fa-89287ee89bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample user: 00000dbacae5abe5e23885899a1fa44253a17956c6d1c3d25f88aa139fdfc657\n",
      "Candidate items example: [156231001 179123001 351484002 372860001 372860002 399223001 399223029\n",
      " 399256001 448509014 464297007 469137001 507909001 524825011 546406001\n",
      " 561445005 562245001 562245046 568597006 568597007 568597009]\n",
      "Any candidate in test week? True\n",
      "GT: []\n",
      "Pred: [568601006, 568597006, 795440004, 814762001, 785515002, 568601026, 793911001, 568597023, 568601007, 568601038, 795440007, 814766001]\n"
     ]
    }
   ],
   "source": [
    "# DEBUG 1: Check candidate items are real article_ids\n",
    "sample_user = list(candidates.keys())[0]\n",
    "print(\"Sample user:\", sample_user)\n",
    "print(\"Candidate items example:\", candidates[sample_user][:20])\n",
    "\n",
    "# DEBUG 2: Check if these items exist in test ground truth articles\n",
    "test_articles = set(test_df[\"article_id\"].unique())\n",
    "print(\"Any candidate in test week?\",\n",
    "      len(set(candidates[sample_user]).intersection(test_articles)) > 0)\n",
    "\n",
    "# DEBUG 3: Check if predictions for sample user overlap ground truth\n",
    "print(\"GT:\", gt.get(sample_user, []))\n",
    "print(\"Pred:\", predictions.get(sample_user, []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea10c7-50f9-42d3-9b01-dcb6bf3f58af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
